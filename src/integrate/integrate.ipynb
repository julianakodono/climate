{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e Configura√ß√µes\n",
    "\n",
    "1. Requer:\n",
    "    * `./data/GlobalLandTemperaturesByCity.csv`;\n",
    "    * `./data/co2_data.csv` ou `./out/co2_data.csv`.\n",
    "\n",
    "<br />\n",
    "\n",
    "2. √â necess√°rio possuir o **Java SE Development Kit** (JDK) instalado;\n",
    "    * Possivel baixar atrav√©s desse *[link](https://www.oracle.com/br/java/technologies/downloads/)*\n",
    "\n",
    "<br />\n",
    "\n",
    "3. Al√©m disso, √© necess√°rio definir a vari√°vel de ambiente `JAVA_HOME` cujo valor √© o diret√≥rio de instala√ß√£o do JDK;\n",
    "    * Exemplo: `C:\\Progra~1\\Java\\jdk-19`;\n",
    "    * Em que `Progra~1` √©, na verdade, a pasta `Program Files`.\n",
    "\n",
    "<br />\n",
    "\n",
    "4. Definir tamb√©m as vari√°veis de ambiente:\n",
    "    * `PYSPARK_DRIVER_PYTHON` = `jupyter`;\n",
    "    * `PYSPARK_PYTHON` = `python`.\n",
    "\n",
    "<br />\n",
    "\n",
    "5. Provavelmente ser√° necess√°rio reiniciar o computador ap√≥s definir essas vari√°veis de ambiente.\n",
    "    * H√° abaixo uma verifica√ß√£o para atestar se foi poss√≠vel l√™-las."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ_vars = ['JAVA_HOME', 'PYSPARK_DRIVER_PYTHON', 'PYSPARK_PYTHON']\n",
    "\n",
    "for var in environ_vars:\n",
    "    if not os.environ.get(var):\n",
    "        print(f'AVISO: Vari√°vel {var} n√£o encontrada.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/d:/Files/Poli/PTC/climate/src/integrate/out/co2_data.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m tpr_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39moptions(header\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcsv(\u001b[39m'\u001b[39m\u001b[39m./data/GlobalLandTemperaturesByCity.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mtry\u001b[39;00m: \n\u001b[1;32m----> 6\u001b[0m     co2_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49moptions(header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTrue\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39m./out/co2_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     co2_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39moptions(header\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcsv(\u001b[39m'\u001b[39m\u001b[39m./data/co2_data.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Files\\Poli\\PTC\\climate\\env\\lib\\site-packages\\pyspark\\sql\\readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    534\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[0;32m    536\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    538\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32md:\\Files\\Poli\\PTC\\climate\\env\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32md:\\Files\\Poli\\PTC\\climate\\env\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/d:/Files/Poli/PTC/climate/src/integrate/out/co2_data.csv"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "tpr_data = spark.read.options(header='True').csv('./data/GlobalLandTemperaturesByCity.csv')\n",
    "\n",
    "try: \n",
    "    co2_data = spark.read.options(header='True').csv('./out/co2_data.csv')\n",
    "except FileNotFoundError:\n",
    "    co2_data = spark.read.options(header='True').csv('./data/co2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|√Örhus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|√Örhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|√Örhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|√Örhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|√Örhus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpr_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset* de temperaturas:\n",
    "\n",
    "* `dt` (string): data da amostra;\n",
    "* `AverageTemperature` (float): temperatura m√©dia medida;\n",
    "* `AverageTemperatureUncertainty` (float): incerteza associada a temperatura medida;\n",
    "* `City` (string): cidade de origem da amostra;\n",
    "* `Country` (string): pa√≠s de origem da amostra;\n",
    "* `Latitude` (string): graus de latitude norte da amostra;\n",
    "* `Longitude` (string):  graus de longitude leste da amostra;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----------------+\n",
      "|     Times|LatDim|LonDim|            value|\n",
      "+----------+------+------+-----------------+\n",
      "|1850-01-01|     0|     0|288.1340637207031|\n",
      "|1850-01-01|     0|     1|288.1340637207031|\n",
      "|1850-01-01|     0|     2|288.1340637207031|\n",
      "|1850-01-01|     0|     3|288.1340637207031|\n",
      "|1850-01-01|     0|     4|288.1340637207031|\n",
      "+----------+------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "co2_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset* de temperaturas:\n",
    "\n",
    "* `Times` (string): data da amostra;\n",
    "* `LatDim` (int): graus de latitude norte da amostra;\n",
    "* `LonDim` (int): graus de longitude leste da amostra;\n",
    "* `value` (float): concentra√ß√£o de $CO_2$ em ppm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento\n",
    "\n",
    "* Define uma mesma data de in√≠cio para ambos os datasets\n",
    "* Realiza corre√ß√£o de tipos de dados e padroniza colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padroniza datasets iniciando em 1850-01-01\n",
    "tpr_data = tpr_data[tpr_data['dt'] >= '1850-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padroniza nomes de colunas entre datasets\n",
    "\n",
    "trp_columns_rename = [\n",
    "    ('dt', 't_date'),\n",
    "    ('AverageTemperature', 't_temperature'),\n",
    "    ('AverageTemperatureUncertainty', 't_temperature_unc'),\n",
    "    ('City', 't_city'),\n",
    "    ('Country', 't_country'),\n",
    "    ('Latitude', 't_latitude'),\n",
    "    ('Longitude', 't_longitude'),\n",
    "]\n",
    "\n",
    "co2_columns_rename = [\n",
    "    ('Times', 'c_date'),\n",
    "    ('LatDim', 'c_latitude'),\n",
    "    ('LonDim', 'c_longitude'),\n",
    "    ('value', 'c_co2'),\n",
    "]\n",
    "\n",
    "for old, new in trp_columns_rename:\n",
    "    tpr_data = tpr_data.withColumnRenamed(old, new)\n",
    "\n",
    "for old, new in co2_columns_rename:\n",
    "    co2_data = co2_data.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retira 'N' e 'E' do final dos valores de \n",
    "# latitude/longitude e converte para float\n",
    "\n",
    "to_strip = ['t_latitude', 't_longitude']\n",
    "\n",
    "for col in to_strip:\n",
    "    tpr_data = tpr_data.withColumn(\n",
    "        col,\n",
    "        expr(f\"float(substring({col}, 1, length({col})-1))\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+------+---------+----------+-----------+\n",
      "|    t_date|       t_temperature|t_temperature_unc|t_city|t_country|t_latitude|t_longitude|\n",
      "+----------+--------------------+-----------------+------+---------+----------+-----------+\n",
      "|1850-01-01|              -5.265|             1.82| √Örhus|  Denmark|     57.05|      10.33|\n",
      "|1850-02-01|               1.859|            1.641| √Örhus|  Denmark|     57.05|      10.33|\n",
      "|1850-03-01|0.031999999999999806|            3.167| √Örhus|  Denmark|     57.05|      10.33|\n",
      "|1850-04-01|  5.7639999999999985|            1.903| √Örhus|  Denmark|     57.05|      10.33|\n",
      "|1850-05-01|              11.037|            0.586| √Örhus|  Denmark|     57.05|      10.33|\n",
      "+----------+--------------------+-----------------+------+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpr_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------------+\n",
      "|    c_date|c_latitude|c_longitude|            c_co2|\n",
      "+----------+----------+-----------+-----------------+\n",
      "|1850-01-01|         0|          0|288.1340637207031|\n",
      "|1850-01-01|         0|          1|288.1340637207031|\n",
      "|1850-01-01|         0|          2|288.1340637207031|\n",
      "|1850-01-01|         0|          3|288.1340637207031|\n",
      "|1850-01-01|         0|          4|288.1340637207031|\n",
      "+----------+----------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "co2_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integraliza√ß√£o\n",
    "\n",
    "* Une datasets pelo crit√©rio de dist√¢ncia m√≠nima\n",
    "\n",
    "    1. Para cada ponto de tempetura, obt√©m a sua data e encontra todos os pontos de $CO_2$ nessa mesma data;\n",
    "    2. Calcula a dist√¢ncia entre o ponto de temperatura e todos os outros pontos de $CO_2$ anteriormente selecionados;\n",
    "    3. Escolhe aquele de m√≠nima dist√¢ncia como o ponto equivalente entre os *datasets*.\n",
    "\n",
    "<br />\n",
    "\n",
    "* C√°lculo da dist√¢ncia\n",
    "    * ‚úîÔ∏è **Abordagem 1:** plana ‚Äî $d(P_1, P_2) = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$\n",
    "    * ‚ùì **Abordagem 2:** esf√©rica ‚Äî Faz sentido? √â necess√°ria? Como fazer?\n",
    "\n",
    "<br />\n",
    "\n",
    "* Sobre a implementa√ß√£o\n",
    "    * N√£o encontrei uma forma direta e elegante para fazer a integra√ß√£o apenas atrav√©s da API do PySpark, ent√£o optei por apelar ao SQL üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorte √≠nfimo dos datasets apenas para valida√ß√£o\n",
    "\n",
    "tpr_data.limit(int(1e3)).createOrReplaceTempView(\"tpr_data\")\n",
    "co2_data.limit(int(1e5)).createOrReplaceTempView(\"co2_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw query de integraliza√ß√£o\n",
    "\n",
    "cross_data = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        -- Ordena√ß√£o crescente das dist√¢ncias,\n",
    "        -- orientado a data, latitude e longitude\n",
    "        SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY\n",
    "                    t_date,\n",
    "                    t_latitude,\n",
    "                    t_longitude\n",
    "                ORDER BY distance ASC\n",
    "            ) AS row_num\n",
    "        FROM (\n",
    "            -- CROSS JOIN para c√°lculo de dist√¢ncias entre\n",
    "            -- todos os pontos dos datasets em uma mesma data\n",
    "            SELECT\n",
    "                *,\n",
    "                SQRT(POW(t_latitude - c_latitude, 2) + POW(t_longitude - c_longitude, 2)) AS distance\n",
    "            FROM tpr_data AS T\n",
    "            CROSS JOIN co2_data AS C\n",
    "            ON t_date = c_date\n",
    "        )\n",
    "    )\n",
    "    WHERE row_num = 1; -- Escolha dos registros com menor dist√¢ncia\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------------+------+---------+----------+-----------+----------+----------+-----------+-----------------+-------------------+-------+\n",
      "|    t_date|t_temperature|t_temperature_unc|t_city|t_country|t_latitude|t_longitude|    c_date|c_latitude|c_longitude|            c_co2|           distance|row_num|\n",
      "+----------+-------------+-----------------+------+---------+----------+-----------+----------+----------+-----------+-----------------+-------------------+-------+\n",
      "|1850-01-01|       -5.265|             1.82| √Örhus|  Denmark|     57.05|      10.33|1850-01-01|        57|         10|285.5076904296875|0.33376619564095866|      1|\n",
      "|1850-02-01|        1.859|            1.641| √Örhus|  Denmark|     57.05|      10.33|1850-02-01|        57|         10|286.4536437988281|0.33376619564095866|      1|\n",
      "+----------+-------------+-----------------+------+---------+----------+-----------+----------+----------+-----------+-----------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07d19eb70bcfc2f0213fb2cc4f0e2b7647738b1a23b2bbd4e03e9ce97554aed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
